{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJeGqIEFnGZn",
        "outputId": "ba8e8446-2a83-4d55-bef9-c3fff9c88ae7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 26 not upgraded.\n",
            "Requirement already satisfied: websockets<15.0,>=13.0 in /usr/local/lib/python3.11/dist-packages (14.2)\n"
          ]
        }
      ],
      "source": [
        "!apt-get update -qq\n",
        "!apt-get install ffmpeg -y\n",
        "!pip install --upgrade \"websockets>=13.0,<15.0\"\n",
        "!pip install --upgrade gradio --quiet\n",
        "!pip install --upgrade openai --quiet\n",
        "!pip install transformers==4.29.0 torch torchvision --quiet\n",
        "!pip install huggingface_hub --quiet\n",
        "!pip install -U openai-whisper --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNijMWE08dH6",
        "outputId": "1b1decbb-89de-45c3-baf3-0e00519fc893"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py:454: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  return torch.load(checkpoint_file, map_location=\"cpu\")\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import json\n",
        "import math\n",
        "import gradio\n",
        "import openai\n",
        "import torch\n",
        "import whisper\n",
        "from PIL import Image\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from openai import OpenAI\n",
        "\n",
        "# openai.api_key = \"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\"\n",
        "client = OpenAI(api_key=\"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\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "whisper_model = whisper.load_model(\"base\", device=device)\n",
        "\n",
        "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "\n",
        "HIERARCHY_TAGS = {\"product\",\"category\",\"industry\",\"brand\",\"none\"}\n",
        "STORYLINE_TAGS = {\"unboxing\",\"testimonial\",\"before-after\",\"tutorial\",\"listicle\",\"daily-routine\",\n",
        "                  \"voice-over-showcase\",\"dialogue\",\"replicate-ad\",\"demonstration\",\"none\"}\n",
        "HOOK_TAGS = {\"strong-reaction\",\"dramatize-problem\",\"absurd-alternative\",\"visual-trick\",\n",
        "             \"highlight-popularity\",\"target-audience-callout\",\"controversy\",\"emphasize-one-usp\",\"none\"}\n",
        "CTA_TAGS = {\"buy_now\",\"download_now\",\"visit_website\",\"sign_up\",\"subscribe\",\"start_free_trial\",\"learn_more\",\"none\"}\n",
        "ICP_TAGS = {\"moms\",\"athletes\",\"students\",\"none\"}\n",
        "ACTOR_TAGS = {\"male\",\"female\",\"mixed\",\"none\"}\n",
        "\n",
        "\n",
        "clip_candidate_texts = [\n",
        "    \"a female actor speaking\",\n",
        "    \"a male actor speaking\",\n",
        "    \"no people visible\",\n",
        "    \"someone unboxing a product\",\n",
        "    \"someone giving a personal testimonial\",\n",
        "    \"someone showing a before-after scenario\",\n",
        "    \"someone teaching a tutorial\",\n",
        "    \"someone in a daily routine\",\n",
        "    \"multiple people in a dialogue\",\n",
        "    \"someone demonstrating a product\",\n",
        "    \"someone with a strong reaction\",\n",
        "    \"someone dramatizing a problem\",\n",
        "    \"someone highlighting brand story\",\n",
        "    # you can add or remove as needed\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2Lrkko1GOP9H"
      },
      "outputs": [],
      "source": [
        "\n",
        "def extract_frames(video_path):\n",
        "    cmd = f'ffprobe -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 \"{video_path}\"'\n",
        "    try:\n",
        "        dur_str = subprocess.check_output(cmd, shell=True).decode(\"utf-8\").strip()\n",
        "        duration = float(dur_str)\n",
        "    except:\n",
        "        duration = 10.0\n",
        "\n",
        "    fixed_times = [0,4,8,12,16,20,24,28]\n",
        "    out_files = []\n",
        "    for i, t in enumerate(fixed_times):\n",
        "        if t >= duration - 0.5:\n",
        "            break\n",
        "        out_f = f\"frame_{i}.jpg\"\n",
        "        cmd = f'ffmpeg -ss {t} -i \"{video_path}\" -frames:v 1 -q:v 2 \"{out_f}\" -y'\n",
        "        os.system(cmd)\n",
        "        if os.path.exists(out_f):\n",
        "            out_files.append(out_f)\n",
        "    return out_files\n",
        "\n",
        "\n",
        "def extract_audio_transcript(video_path):\n",
        "    wav_file = \"temp_audio.wav\"\n",
        "    cmd = f'ffmpeg -i \"{video_path}\" -q:a 0 -map a \"{wav_file}\" -y'\n",
        "    # cmd = f'ffmpeg -i \"{video_path}\" -vn -acodec pcm_s16le -ar 16000 -ac 1 \"{wav_file}\" -y'\n",
        "\n",
        "    # Run the command and capture output\n",
        "    try:\n",
        "        subprocess.run(cmd, shell=True, check=True, capture_output=True)\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"FFmpeg command failed with error: {e.stderr.decode()}\")\n",
        "        return None\n",
        "\n",
        "    # Check if the WAV file was created\n",
        "    if not os.path.exists(wav_file):\n",
        "        print(f\"Error: {wav_file} was not created.\")\n",
        "        return None\n",
        "\n",
        "    # Transcribe the audio\n",
        "    try:\n",
        "        result = whisper_model.transcribe(wav_file)\n",
        "        return result[\"text\"]\n",
        "    except Exception as e:\n",
        "        print(f\"Whisper transcription failed: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def analyze_frame_with_clip(frame_path):\n",
        "    im = Image.open(frame_path).convert(\"RGB\")\n",
        "    inputs = clip_processor(text=clip_candidate_texts, images=im, return_tensors=\"pt\", padding=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = clip_model(**inputs)\n",
        "    logits_per_image = outputs.logits_per_image\n",
        "    probs = logits_per_image.softmax(dim=1)[0].tolist()\n",
        "\n",
        "    max_prob = max(probs)\n",
        "    max_idx = probs.index(max_prob)\n",
        "    chosen_label = clip_candidate_texts[max_idx]\n",
        "    return chosen_label, max_prob\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "SYSTEM_PROMPT_HEAD = \"\"\"\n",
        "You are an ad classification assistant.\n",
        "We have EXACTLY 6 keys: \"hierarchy\",\"storyline\",\"hook\",\"cta\",\"icp\",\"actor\".\n",
        "Each key must be a single string from the sets below, no arrays or multiple strings:\n",
        "\n",
        "hierarchy: {hierarchy}\n",
        "storyline: {storyline}\n",
        "hook: {hook}\n",
        "cta: {cta}\n",
        "icp: {icp}\n",
        "actor: {actor}\n",
        "\n",
        "If the ad is borderline, pick the single best match. Return valid JSON with these 6 keys.\n",
        "No extra commentary, no arrays.\n",
        "\"\"\"\n",
        "\n",
        "SYSTEM_PROMPT_HEAD = SYSTEM_PROMPT_HEAD.format(\n",
        "    hierarchy=list(HIERARCHY_TAGS),\n",
        "    storyline=list(STORYLINE_TAGS),\n",
        "    hook=list(HOOK_TAGS),\n",
        "    cta=list(CTA_TAGS),\n",
        "    icp=list(ICP_TAGS),\n",
        "    actor=list(ACTOR_TAGS),\n",
        ")\n",
        "EXAMPLE_1 = \"\"\"\n",
        "EXAMPLE 1:\n",
        "Transcript:\n",
        "\"I just opened this brand new box, let's see what's inside!\n",
        "So excited to discover everything for the first time.\"\n",
        "\n",
        "Frame Observations:\n",
        "frame_0: \"someone unboxing a product\"\n",
        "frame_1: \"a female actor speaking\"\n",
        "\n",
        "FINAL classification:\n",
        "{\n",
        "  \"hierarchy\": \"product\",\n",
        "  \"storyline\": \"unboxing\",\n",
        "  \"hook\": \"none\",\n",
        "  \"cta\": \"none\",\n",
        "  \"icp\": \"none\",\n",
        "  \"actor\": \"female\"\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "EXAMPLE_2 = \"\"\"\n",
        "EXAMPLE 2:\n",
        "Transcript:\n",
        "\"I have used this cream for 2 months, let me show you my before-and-after results.\n",
        "I'm stunned by the difference.\"\n",
        "\n",
        "Frame Observations:\n",
        "frame_0: \"someone showing a before-after scenario\"\n",
        "frame_1: \"a female actor speaking\"\n",
        "\n",
        "FINAL classification:\n",
        "{\n",
        "  \"hierarchy\": \"product\",\n",
        "  \"storyline\": \"before-after\",\n",
        "  \"hook\": \"emphasize-one-usp\",\n",
        "  \"cta\": \"none\",\n",
        "  \"icp\": \"none\",\n",
        "  \"actor\": \"female\"\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "EXAMPLE_3 = \"\"\"\n",
        "EXAMPLE 3:\n",
        "Transcript:\n",
        "\"This sale is insaneâ€”20% discount for new moms only!\n",
        "We dramatize the problem if you keep using old products,\n",
        "sign up now to see the difference.\"\n",
        "\n",
        "Frame Observations:\n",
        "frame_0: \"someone dramatizing a problem\"\n",
        "frame_1: \"a female actor speaking\"\n",
        "\n",
        "FINAL classification:\n",
        "{\n",
        "  \"hierarchy\": \"product\",\n",
        "  \"storyline\": \"testimonial\",\n",
        "  \"hook\": \"dramatize-problem\",\n",
        "  \"cta\": \"sign_up\",\n",
        "  \"icp\": \"moms\",\n",
        "  \"actor\": \"female\"\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "SYSTEM_PROMPT_EXAMPLES = EXAMPLE_1 + \"\\n\" + EXAMPLE_2 + \"\\n\" + EXAMPLE_3\n",
        "\n",
        "SYSTEM_PROMPT_TAIL = \"\"\"\n",
        "Now I will give you the real transcript + frame observations.\n",
        "Use hidden chain-of-thought, but do NOT output it. Output final JSON only, no arrays, no extra fields.\n",
        "\"\"\"\n",
        "\n",
        "def build_system_prompt():\n",
        "    return SYSTEM_PROMPT_HEAD + \"\\n\" + SYSTEM_PROMPT_EXAMPLES + \"\\n\" + SYSTEM_PROMPT_TAIL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "I88DdCPzOSzs"
      },
      "outputs": [],
      "source": [
        "\n",
        "def parse_and_validate_single_string(response_text):\n",
        "    try:\n",
        "        data = json.loads(response_text)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "    expected_keys = [\"hierarchy\",\"storyline\",\"hook\",\"cta\",\"icp\",\"actor\"]\n",
        "    for k in expected_keys:\n",
        "        if k not in data:\n",
        "            return None\n",
        "\n",
        "    # allowed sets\n",
        "    allowed_map = {\n",
        "        \"hierarchy\": HIERARCHY_TAGS,\n",
        "        \"storyline\": STORYLINE_TAGS,\n",
        "        \"hook\": HOOK_TAGS,\n",
        "        \"cta\": CTA_TAGS,\n",
        "        \"icp\": ICP_TAGS,\n",
        "        \"actor\": ACTOR_TAGS\n",
        "    }\n",
        "\n",
        "    for k in expected_keys:\n",
        "        val = data[k]\n",
        "        if not isinstance(val, str):\n",
        "            return None\n",
        "        if val not in allowed_map[k]:\n",
        "            return None\n",
        "\n",
        "    return data\n",
        "\n",
        "def advanced_llm_classify(transcript, clip_info):\n",
        "    \"\"\"\n",
        "    1) Build a big user prompt with transcript + frame observations\n",
        "    2) Use advanced system prompt with chain-of-thought examples\n",
        "    3) parse & if invalid, re-ask once\n",
        "    \"\"\"\n",
        "    system_prompt = build_system_prompt()\n",
        "\n",
        "    # Prepare the user prompt based on whether a transcript is available\n",
        "    if transcript and transcript != \"No audio transcript available.\":\n",
        "        user_prompt = f\"\"\"\n",
        "    Transcript:\n",
        "    {transcript}\n",
        "\n",
        "    Frame Observations:\n",
        "    {clip_info}\n",
        "\n",
        "    Now produce final classification as valid JSON.\n",
        "    Remember, exactly one label from each set, no arrays, no commentary.\n",
        "    \"\"\"\n",
        "    else:\n",
        "        user_prompt = f\"\"\"\n",
        "    No audio transcript is available. Classify the video based on the following frame observations:\n",
        "\n",
        "    Frame Observations:\n",
        "    {clip_info}\n",
        "\n",
        "    Now produce final classification as valid JSON.\n",
        "    Remember, exactly one label from each set, no arrays, no commentary.\n",
        "    \"\"\"\n",
        "    # FIRST attempt\n",
        "    # response1 = openai.ChatCompletion.create(\n",
        "    #     model=\"gpt-3.5-turbo\",\n",
        "    #     messages=[\n",
        "    #         {\"role\":\"system\",\"content\": system_prompt},\n",
        "    #         {\"role\":\"user\",\"content\": user_prompt}\n",
        "    #     ],\n",
        "    #     temperature=0\n",
        "    # )\n",
        "    response1 = client.chat.completions.create(\n",
        "        model=\"gpt-3.5-turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt}\n",
        "        ],\n",
        "        temperature=0\n",
        "    )\n",
        "    raw_text1 = response1.choices[0].message.content\n",
        "\n",
        "    parsed1 = parse_and_validate_single_string(raw_text1)\n",
        "    if parsed1 is not None:\n",
        "        return json.dumps(parsed1, indent=2)\n",
        "    else:\n",
        "        # second attempt\n",
        "        fix_prompt = f\"\"\"\n",
        "Your previous output was invalid or had unknown tags/arrays.\n",
        "We need single-string for each key.\n",
        "Allowed sets are shown above.\n",
        "No arrays or multiple items.\n",
        "Here's your last output:\n",
        "\n",
        "{raw_text1}\n",
        "\n",
        "Fix it and produce valid JSON:\n",
        "\"\"\"\n",
        "        # response2 = openai.ChatCompletion.create(\n",
        "        #     model=\"gpt-3.5-turbo\",\n",
        "        #     messages=[\n",
        "        #         {\"role\":\"system\",\"content\": system_prompt},\n",
        "        #         {\"role\":\"user\",\"content\": fix_prompt}\n",
        "        #     ],\n",
        "        #     temperature=0\n",
        "        # )\n",
        "        response2 = client.chat.completions.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": fix_prompt}\n",
        "            ],\n",
        "            temperature=0\n",
        "        )\n",
        "        raw_text2 = response2.choices[0].message.content\n",
        "        parsed2 = parse_and_validate_single_string(raw_text2)\n",
        "\n",
        "        if parsed2 is not None:\n",
        "            return json.dumps(parsed2, indent=2)\n",
        "        else:\n",
        "            return f\"Still invalid after second attempt.\\n\\nFirst attempt:\\n{raw_text1}\\n\\nSecond attempt:\\n{raw_text2}\"\n",
        "\n",
        "def classify_ad(video):\n",
        "    # if no video\n",
        "    if video is None:\n",
        "        return \"No video provided.\"\n",
        "\n",
        "    # handle path\n",
        "    video_path = video[\"name\"] if isinstance(video, dict) else video\n",
        "\n",
        "\n",
        "\n",
        "    # 1) Transcribe\n",
        "    transcript_text = extract_audio_transcript(video_path)\n",
        "\n",
        "\n",
        "    # 2) Frames\n",
        "    frames = extract_frames(video_path)\n",
        "\n",
        "\n",
        "    # 3) CLIP each frame\n",
        "    clip_obs = []\n",
        "    for f in frames:\n",
        "        label, prob = analyze_frame_with_clip(f)\n",
        "        clip_obs.append(f\"{f}: \\\"{label}\\\" (prob={prob:.2f})\")\n",
        "\n",
        "    clip_info = \"\\n\".join(clip_obs)\n",
        "\n",
        "    # 4) advanced LLM classify\n",
        "    final_tags = advanced_llm_classify(transcript_text, clip_info)\n",
        "    return final_tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "EI6ZTEG4OnSW"
      },
      "outputs": [],
      "source": [
        "def run_app():\n",
        "    import gradio as gr\n",
        "    demo = gr.Interface(\n",
        "        fn=classify_ad,\n",
        "        inputs=gr.Video(label=\"Upload MP4\"),\n",
        "        outputs=\"text\",\n",
        "        title=\"Whisper + CLIP + LLM (Best Version)\",\n",
        "        description=\"\"\"\n",
        "BEST version:\n",
        "- Deterministic frames at 0,4,8,12,16,20,24,28s\n",
        "- Whisper for transcript\n",
        "- CLIP with expanded prompts\n",
        "- GPT-3.5 with multi examples (chain-of-thought hidden)\n",
        "- Single label categories, fallback fix prompt if invalid\n",
        "\"\"\"\n",
        "    )\n",
        "    demo.launch(debug=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 778
        },
        "id": "LrhvJUiFOuZH",
        "outputId": "450a5565-dc00-4964-d1fa-1df2dba55d7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://179932bf95a9a5a773.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://179932bf95a9a5a773.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hi\n",
            "hi\n",
            "hi\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://179932bf95a9a5a773.gradio.live\n"
          ]
        }
      ],
      "source": [
        "run_app()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}